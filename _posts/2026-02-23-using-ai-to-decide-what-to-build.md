---
layout: post
title: "I Asked My AI Agent What We Should Build Next. Here's What Happened."
date: 2026-02-23
categories: [essays]
published: false
---

We kept building the wrong things.

Not broken things. Impressive things.

Cool demos. Fast prototypes. Features that looked great in a thread and did nothing for the business.

The hardest part of AI isn't building. It's choosing.

So we tried something different.

We asked the agent what we should build next.

---

## The problem

Small teams don't get many bets.

One wrong build can burn a month. Two wrong builds can burn a quarter. You can stay busy the whole time and still move backward.

The normal playbook wasn't helping:

- Market research was slow and expensive.
- X was noisy and addicted to hype.
- Customer calls were gold, but only for known pain.
- Competitor analysis assumes you already know the field.

We needed broader signal. Fast.

Not vibes. Signal.

---

## The approach

We gave the agent a narrow brief:

> "Survey the current AI tools landscape for small businesses and agencies. Focus on: what tools exist, what problems they claim to solve, what the most common complaints are in reviews and forums, and where there are obvious gaps between what users need and what's currently available. Prioritize Reddit, HN, G2, and Capterra. Return a structured gap analysis."

Then let it run multi-step research: search, read, cluster complaints, synthesize.

No one-shot magic prompt. Just disciplined grunt work at machine speed.

It came back with a structured report.

Better than expected.

---

## What came back

Three buckets: **saturated**, **emerging**, **underserved**.

**Saturated** (skip): AI writing tools, chatbot builders, meeting summarizers.

No surprise. Too many players. Too little edge.

**Emerging** (watch): AI-native CRM, field-service ops tooling, agent-to-agent workflows.

Messy category. Early products. A lot of "works in demo" energy.

**Underserved** (interesting):

1. **Integration as the product.** Not another wrapper. Real connective tissue across ugly stacks (Sheets + legacy CRM + duct-taped email).
2. **Explainability for non-technical teams.** Owners trusted outputs but couldn't defend decisions to stakeholders.
3. **Intake-layer tooling.** Proposals, scoping, onboarding. High pain. Thin tooling.

That's where the air was.

---

## Signal vs. noise

Honest split: about 60% was obvious.

"Writing tools are crowded" wasn't exactly a revelation.

But seeing the obvious written as structured evidence changed the conversation. Harder to ignore. Easier to act.

The other 40% was the win.

Not because it was hidden. Because we weren't looking there.

The field-service angle is a good example. We knew it was messy. We didn't know how consistently people were complaining across channels.

Same pain. Different people. Different geos. Same wording.

That's not noise.

That's a market telling you where it's still broken.

---

## The meta point

Yes, we used AI to decide what AI to build.

A little ridiculous. Also extremely practical.

If you're building tools and not running them on your own decisions first, you're skipping your best feedback loop.

One caveat: this only worked because the agent had context from real client work. Without context, you get generic output dressed up as insight.

The report wasn't a roadmap.

It was a sharper set of questions.

And when you're deciding what to build next, better questions are the whole game.

We picked the intake-layer tool.

Akino Solar is the first real test.

That's the loop.
